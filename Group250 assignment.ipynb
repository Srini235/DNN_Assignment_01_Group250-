{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srini235/DNN_Assignment_01_Group250-/blob/main/Group250%20assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64-XCLdSBBuc"
      },
      "source": [
        "# Deep Neural Networks - Programming Assignment\n",
        "## Comparing Linear Models and Multi-Layer Perceptrons\n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "\n",
        "**Date:** ___________________\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ IMPORTANT INSTRUCTIONS\n",
        "\n",
        "1. **Complete ALL sections** marked with `TODO`\n",
        "2. **DO NOT modify** the `get_assignment_results()` function structure\n",
        "3. **Track training time** for both models using `time.time()`\\n\n",
        "4. **Store loss_history** in both model classes\n",
        "5. **Calculate ALL metrics** (accuracy, precision, recall, F1)\n",
        "6. **Fill get_assignment_results()** with ALL required fields\n",
        "7. **PRINT the results** - Auto-grader needs visible output!\n",
        "8. **Run all cells** before submitting (Kernel → Restart & Run All)\n",
        "\n",
        "**SCORING:**\n",
        "- Missing fields = 0 marks for that section\n",
        "- Non-executed notebook = 0 marks\n",
        "- Cleared outputs = 0 marks\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (2.2.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: six>=1.5 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (3.10.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pillow>=8 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: six>=1.5 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (1.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.22.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from scikit-learn) (2.2.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ucimlrepo in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from ucimlrepo) (2.3.3)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from ucimlrepo) (2025.11.12)\n",
            "Requirement already satisfied: numpy>=1.22.4 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.2.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2025.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3Pti5nJuBBue"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import datasets\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "print('Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNyOCAuwBBue"
      },
      "source": [
        "## Section 1: Dataset Selection and Loading\n",
        "\n",
        "**Requirements:**\n",
        "- ≥500 samples\n",
        "- ≥5 features\n",
        "- Public dataset (UCI/Kaggle)\n",
        "- Regression OR Classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'uci_id': 891, 'name': 'CDC Diabetes Health Indicators', 'repository_url': 'https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators', 'data_url': 'https://archive.ics.uci.edu/static/public/891/data.csv', 'abstract': 'The Diabetes Health Indicators Dataset contains healthcare statistics and lifestyle survey information about people in general along with their diagnosis of diabetes. The 35 features consist of some demographics, lab test results, and answers to survey questions for each patient. The target variable for classification is whether a patient has diabetes, is pre-diabetic, or healthy. ', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Tabular', 'Multivariate'], 'num_instances': 253680, 'num_features': 21, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Sex', 'Age', 'Education Level', 'Income'], 'target_col': ['Diabetes_binary'], 'index_col': ['ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2017, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C53919', 'creators': [], 'intro_paper': {'ID': 308, 'type': 'NATIVE', 'title': 'Incidence of End-Stage Renal Disease Attributed to Diabetes Among Persons with Diagnosed Diabetes — United States and Puerto Rico, 2000–2014', 'authors': 'Nilka Rios Burrows, MPH; Israel Hora, PhD; Linda S. Geiss, MA; Edward W. Gregg, PhD; Ann Albright, PhD', 'venue': 'Morbidity and Mortality Weekly Report', 'year': 2017, 'journal': None, 'DOI': None, 'URL': 'https://www.cdc.gov/mmwr/volumes/66/wr/mm6643a2.htm', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Dataset link: https://www.cdc.gov/brfss/annual_data/annual_2014.html', 'purpose': 'To better understand the relationship between  lifestyle and diabetes in the US', 'funded_by': 'The CDC', 'instances_represent': 'Each row represents a person participating in this study.', 'recommended_data_splits': 'Cross validation or a fixed train-test split could be used.', 'sensitive_data': '- Gender\\n- Income\\n- Education level', 'preprocessing_description': 'Bucketing of age', 'variable_info': '- Diabetes diagnosis\\n- Demographics (race, sex)\\n- Personal information (income, educations)\\n- Health history (drinking, smoking, mental health, physical health)', 'citation': None}, 'external_url': 'https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset'}\n",
            "(253680, 1)\n"
          ]
        }
      ],
      "source": [
        "# # Load your dataset\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo \n",
        "  \n",
        "# fetch dataset \n",
        "cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
        "  \n",
        "# data (as pandas dataframes) \n",
        "x = cdc_diabetes_health_indicators.data.features \n",
        "y = cdc_diabetes_health_indicators.data.targets \n",
        "  \n",
        "# metadata \n",
        "print(cdc_diabetes_health_indicators.metadata) \n",
        "  \n",
        "\n",
        "# x, y = datasets.load_wine(return_X_y=True, as_frame=True)\n",
        "x = x.to_numpy()\n",
        "y = y.to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(253680, 1)\n",
            "(253680,)\n"
          ]
        }
      ],
      "source": [
        "print(y.shape)\n",
        "y = y.flatten()\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9QfsHoIkBBuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: CDC Diabetes Health Indicators\n",
            "Source: UCI ML Repository\n",
            "Samples: 253680, Features: 21\n",
            "Problem Type: multiclass_classification\n",
            "Primary Metric: recall\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load your dataset\n",
        "# Example: data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Dataset information (TODO: Fill these)\n",
        "dataset_name = \"CDC Diabetes Health Indicators\"  # e.g., \"Breast Cancer Wisconsin\"\n",
        "dataset_source = \"UCI ML Repository\"  # e.g., \"UCI ML Repository\"\n",
        "n_samples = 253680      # Total number of rows\n",
        "n_features = x.shape[1]     # Number of features (excluding target)\n",
        "problem_type = \"multiclass_classification\"  # \"regression\" or \"binary_classification\" or \"multiclass_classification\"\n",
        "\n",
        "# Problem statement (TODO: Write 2-3 sentences)\n",
        "problem_statement = \"\"\"\n",
        "Predicting diabetes health indicators from patient data.\n",
        "This is critical for early intervention and management of diabetes in healthcare settings.\n",
        "\"\"\"\n",
        "\n",
        "# Primary evaluation metric (TODO: Fill this)\n",
        "primary_metric = \"recall\"  # e.g., \"recall\", \"accuracy\", \"rmse\", \"r2\"\n",
        "# Metric justification (TODO: Write 2-3 sentences)\n",
        "metric_justification = \"\"\"\n",
        "I chose recall because in medical diagnosis,\n",
        "false negatives (missing diabetes cases) are more costly than false positives.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Source: {dataset_source}\")\n",
        "print(f\"Samples: {n_samples}, Features: {n_features}\")\n",
        "print(f\"Problem Type: {problem_type}\")\n",
        "print(f\"Primary Metric: {primary_metric}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XOI6I6JBBuf"
      },
      "source": [
        "## Section 2: Data Preprocessing\n",
        "\n",
        "Preprocess your data:\n",
        "1. Handle missing values\n",
        "2. Encode categorical variables\n",
        "3. Split into train/test sets\n",
        "4. Scale features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mfQProeUBBuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 202944\n",
            "Test samples: 50736\n",
            "Split ratio: 80.0%\n"
          ]
        }
      ],
      "source": [
        "# TODO: Preprocess your data\n",
        "# 1. Separate features (X) and target (y)\n",
        "# 2. Handle missing values if any\n",
        "# 3. Encode categorical variables\n",
        "\n",
        "# Example:\n",
        "# X = data.drop('target', axis=1)\n",
        "# y = data['target']\n",
        "\n",
        "# TODO: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fill these after preprocessing\n",
        "train_samples = len(y_train)       # Number of training samples\n",
        "test_samples = len(y_test)        # Number of test samples\n",
        "train_test_ratio = 0.8  # e.g., 0.8 for 80-20 split\n",
        "\n",
        "print(f\"Train samples: {train_samples}\")\n",
        "print(f\"Test samples: {test_samples}\")\n",
        "print(f\"Split ratio: {train_test_ratio:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elw6-stdBBuf"
      },
      "source": [
        "## Section 3: Baseline Model Implementation\n",
        "\n",
        "Implement from scratch (NO sklearn models!):\n",
        "- Linear Regression (for regression)\n",
        "- Logistic Regression (for binary classification)\n",
        "- Softmax Regression (for multiclass classification)\n",
        "\n",
        "**Must include:**\n",
        "- Forward pass (prediction)\n",
        "- Loss computation\n",
        "- Gradient computation\n",
        "- Gradient descent loop\n",
        "- Loss tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bvQ2rWfWBBuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Baseline model class defined\n"
          ]
        }
      ],
      "source": [
        "class BaselineModel:\n",
        "    \"\"\"\n",
        "    Baseline linear model with gradient descent\n",
        "    Implement: Linear/Logistic/Softmax Regression\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.05, n_iterations=1000, batch_size=32):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size # New parameter for mini-batch\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        # Automatically determine number of classes for multiclass classification\n",
        "        if problem_type == \"multiclass_classification\":\n",
        "            self.n_classes = len(np.unique(y))\n",
        "\n",
        "    def xavier_init(self, n_features, n_classes):\n",
        "        \"\"\"Xavier/Glorot initialization for weights\"\"\"\n",
        "        limit = np.sqrt(6 / (n_features + n_classes))\n",
        "        return np.random.uniform(-limit, limit, (n_features, n_classes))\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"\n",
        "        Compute softmax probabilities for multiclass classification\"\"\"\n",
        "        logits = np.dot(X, self.weights) + self.bias\n",
        "        exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        y_pred = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "        return y_pred\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        TODO: Implement gradient descent training\n",
        "\n",
        "        Steps:\n",
        "        1. Initialize weights and bias\n",
        "        2. For each iteration:\n",
        "           a. Compute predictions (forward pass)\n",
        "           b. Compute loss\n",
        "           c. Compute gradients\n",
        "           d. Update weights and bias\n",
        "           e. Store loss in self.loss_history\n",
        "\n",
        "        Must populate self.loss_history with loss at each iteration!\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Xavier/Glorot initialization for multiclass classification\n",
        "        if problem_type == \"multiclass_classification\":\n",
        "            #self.weights = self.xavier_init(n_features, self.n_classes)\n",
        "            self.weights = np.zeros((n_features, self.n_classes))\n",
        "        else:\n",
        "            self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        y_one_hot = np.eye(self.n_classes)[y.astype(int)] if problem_type == \"multiclass_classification\" else y\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # TODO: Implement gradient descent loop\n",
        "        # Mini batch gradient descent\n",
        "        for epoch in range(self.n_iterations // self.batch_size):\n",
        "            # Shuffle the data at the beginning of each epoch\n",
        "            indices = np.arange(n_samples)\n",
        "            np.random.shuffle(indices)\n",
        "            X_shuffled = X[indices]\n",
        "            y_one_hot_shuffled = y_one_hot[indices]\n",
        "            # Mini-batch processing\n",
        "            for epoch_iter in range(0, n_samples, self.batch_size):\n",
        "                end_idx = min(epoch_iter + self.batch_size, n_samples)\n",
        "                X_batch = X_shuffled[epoch_iter:end_idx]\n",
        "                y_one_hot_batch = y_one_hot_shuffled[epoch_iter:end_idx]\n",
        "                current_batch_size = X_batch.shape[0]\n",
        "\n",
        "                if current_batch_size == 0:\n",
        "                    continue  # Skip empty batch\n",
        "\n",
        "                # 1. Forward pass: (for multiclass classification, use softmax)\n",
        "\n",
        "                # Compute logits (linear output)\n",
        "                logits = np.dot(X_batch, self.weights) + self.bias\n",
        "                # Softmax for multiclass classification\n",
        "                # Find exponential scores\n",
        "                exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "                # Softmax probabilities\n",
        "                y_pred = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "                # 2. Compute categorical cross-entropy loss\n",
        "                # Clip probabilities to avoid log(0) for numerical stability\n",
        "                y_pred_clipped = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
        "                loss = -np.sum(y_one_hot_batch * np.log(y_pred_clipped)) / current_batch_size\n",
        "                epoch_loss += loss\n",
        "                num_batches += 1\n",
        "\n",
        "                # 3. Compute gradients: dw = ..., db = ...\n",
        "                dw = (1 / current_batch_size) * np.dot(X_batch.T, (y_pred_clipped - y_one_hot_batch))\n",
        "                db = (1 / current_batch_size) * np.sum(y_pred_clipped - y_one_hot_batch)\n",
        "                # 4. Update: self.weights -= self.lr * dw\n",
        "                self.weights -= self.lr * dw\n",
        "                self.bias -= self.lr * db\n",
        "                # 5. Append loss to history\n",
        "                self.loss_history.append(loss)\n",
        "            if num_batches > 0:\n",
        "                self.loss_history.append(epoch_loss / num_batches) # Store average loss for the epoch\n",
        "            else:\n",
        "                self.loss_history.append(epoch_loss) # Should only happen if n_samples=0\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Implement prediction\n",
        "\n",
        "        For regression: return linear_output\n",
        "        For classification: return class probabilities or labels\n",
        "        \"\"\"\n",
        "        if self.weights is None or self.bias is None:\n",
        "            raise RuntimeError(\"Model has not been trained yet. Call .fit() first.\")\n",
        "\n",
        "        # Compute logits (linear output)\n",
        "        logits = np.dot(X, self.weights) + self.bias\n",
        "        # Softmax for multiclass classification\n",
        "        # Find exponential scores\n",
        "        exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        # Softmax probabilities\n",
        "        y_pred = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        #probabilities = self._softmax(logits)\n",
        "        return np.argmax(y_pred, axis=1) # Return the class index with the highest probability\n",
        "\n",
        "print(\"✓ Baseline model class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "grpLqkmTBBuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training baseline model...\n",
            "✓ Baseline training completed in 0.49s\n",
            "✓ Loss decreased from 0.6931 to 0.6532\n"
          ]
        }
      ],
      "source": [
        "# Train baseline model\n",
        "print(\"Training baseline model...\")\n",
        "baseline_start = time.time()\n",
        "\n",
        "# TODO: Initialize and train your baseline model\n",
        "baseline_model = BaselineModel(learning_rate=0.01, n_iterations=1000, batch_size=128)\n",
        "baseline_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# TODO: Make predictions\n",
        "baseline_predictions = baseline_model.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "\n",
        "baseline_training_time = time.time() - baseline_start\n",
        "print(f\"✓ Baseline training completed in {baseline_training_time:.2f}s\")\n",
        "print(f\"✓ Loss decreased from {baseline_model.loss_history[0]:.4f} to {baseline_model.loss_history[-1]:.4f}\")\n",
        "\n",
        "# Store loss explicitly\n",
        "baseline_initial_loss = baseline_model.loss_history[0]\n",
        "baseline_final_loss = baseline_model.loss_history[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQpy-zZSBBug"
      },
      "source": [
        "## Section 4: Multi-Layer Perceptron Implementation\n",
        "\n",
        "Implement MLP from scratch with:\n",
        "- At least 1 hidden layer\n",
        "- ReLU activation for hidden layers\n",
        "- Appropriate output activation\n",
        "- Forward propagation\n",
        "- Backward propagation\n",
        "- Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "r2wqZVlfBBug"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron implemented from scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, architecture, learning_rate=0.01, n_iterations=1000):\n",
        "        \"\"\"\n",
        "        architecture: list [input_size, hidden1, hidden2, ..., output_size]\n",
        "        Example: [30, 16, 8, 1] means:\n",
        "            - 30 input features\n",
        "            - Hidden layer 1: 16 neurons\n",
        "            - Hidden layer 2: 8 neurons\n",
        "            - Output layer: 1 neuron\n",
        "        \"\"\"\n",
        "        self.architecture = architecture\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.parameters = {}\n",
        "        self.loss_history = []\n",
        "        self.cache = {}\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        TODO: Initialize weights and biases for all layers\n",
        "\n",
        "        For each layer l:\n",
        "        - W[layer_index]: weight matrix of shape (n[layer_index], n[layer_index-1])\n",
        "        - b[layer_index]: bias vector of shape (n[layer_index], 1)\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "\n",
        "        for layer_index in range(1, len(self.architecture)):\n",
        "            num_perceptrons_current = self.architecture[layer_index]\n",
        "            num_perceptrons_previous = self.architecture[layer_index-1]\n",
        "            # Weights\n",
        "            # # He init for hidden layers; Xavier-ish for output\n",
        "            # if layer_index < len(self.architecture) - 1:\n",
        "            #     self.parameters[f'W{layer_index}'] = np.random.randn(num_perceptrons_current, num_perceptrons_previous) \\\n",
        "            #         * np.sqrt(2/num_perceptrons_previous)\n",
        "            # else:\n",
        "            #     self.parameters[f'W{layer_index}'] = np.random.randn(num_perceptrons_current, num_perceptrons_previous) \\\n",
        "            #         * np.sqrt(1/num_perceptrons_previous)\n",
        "            self.parameters[f'W{layer_index}'] = np.zeros((num_perceptrons_current, num_perceptrons_previous))\n",
        "            # print(f'W{layer_index}: {self.parameters[f'W{layer_index}'].shape}')\n",
        "\n",
        "            # Bias\n",
        "            self.parameters[f'b{layer_index}'] = np.zeros((num_perceptrons_current, 1))\n",
        "            # print(f'b{layer_index}: {self.parameters[f'b{layer_index}'].shape}')\n",
        "\n",
        "\n",
        "\n",
        "    def relu(self, Z):\n",
        "        \"\"\"ReLU activation function\"\"\"\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "        \"\"\"ReLU derivative\"\"\"\n",
        "        return (Z > 0).astype(float)\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        \"\"\"Sigmoid activation (for binary classification output)\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Implement forward pass through all layers\n",
        "\n",
        "        For each layer:\n",
        "        1. Z[l] = W[l] @ A[l-1] + b[l]\n",
        "        2. A[l] = activation(Z[l])\n",
        "\n",
        "        Store Z and A in self.cache for backpropagation\n",
        "        Return final activation A[L]\n",
        "        \"\"\"\n",
        "        fc_in = X.T\n",
        "        self.cache['A0'] = fc_in\n",
        "        num_layers = len(self.architecture) - 1\n",
        "        for layer_index in range(1, num_layers + 1):\n",
        "            weight, bias = self.parameters[f'W{layer_index}'], self.parameters[f'b{layer_index}']\n",
        "            fc_out = np.dot(weight, fc_in) + bias\n",
        "            activation = self.relu(fc_out) if layer_index < num_layers else self.sigmoid(fc_out)\n",
        "            self.cache[f'Z{layer_index}'] = fc_out\n",
        "            self.cache[f'A{layer_index}'] = activation\n",
        "            fc_in = activation\n",
        "        \n",
        "        final_layer_activation = activation\n",
        "        return final_layer_activation\n",
        "\n",
        "        # TODO: Implement forward pass\n",
        "        # for l in range(1, len(self.architecture)):\n",
        "        #     ...\n",
        "\n",
        "    def backward_propagation(self, y):\n",
        "        grads = {}\n",
        "        num_samples = y.shape[0]\n",
        "        num_layers = len(self.architecture) - 1\n",
        "        activation = self.cache[f'A{num_layers}'] \n",
        "        previous_layer_activation = self.cache[f'A{num_layers-1}'] if num_layers-1 >= 0 else self.cache['A0']\n",
        "        dZ_L = (activation - y.reshape(1, num_samples))         \n",
        "        grads[f'dW{num_layers}'] = (1/num_samples) * np.dot(dZ_L, previous_layer_activation.T)\n",
        "        grads[f'db{num_layers}'] = (1/num_samples) * np.sum(dZ_L, axis=1, keepdims=True)\n",
        "\n",
        "        for layer_index in range(num_layers-1, 0, -1):\n",
        "            W_next = self.parameters[f'W{layer_index+1}']\n",
        "            dZ_next = dZ_L if layer_index == num_layers-1 else grads[f'dZ{layer_index+1}']\n",
        "            previous_layer_activation = self.cache[f'A{layer_index-1}'] if layer_index-1 >= 0 else self.cache['A0']\n",
        "            Z_l = self.cache[f'Z{layer_index}']\n",
        "            dA_l = np.dot(W_next.T, dZ_next)\n",
        "            dZ_l = dA_l * self.relu_derivative(Z_l)\n",
        "            grads[f'dZ{layer_index}'] = dZ_l\n",
        "            grads[f'dW{layer_index}'] = (1/num_samples) * np.dot(dZ_l, previous_layer_activation.T)\n",
        "            grads[f'db{layer_index}'] = (1/num_samples) * np.sum(dZ_l, axis=1, keepdims=True)\n",
        "        return grads\n",
        "\n",
        "    def update_parameters(self, grads):\n",
        "        for layer_index in range(1, len(self.architecture)):\n",
        "            self.parameters[f'W{layer_index}'] -= self.lr * grads[f'dW{layer_index}']\n",
        "            self.parameters[f'b{layer_index}'] -= self.lr * grads[f'db{layer_index}']\n",
        "\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "       m = y_true.shape[0]\n",
        "       eps = 1e-12\n",
        "       y_pred = np.clip(y_pred.flatten(), eps, 1-eps)\n",
        "       return -(1/m) * np.sum(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        For each iteration:\n",
        "        1. Forward propagation\n",
        "        2. Compute loss\n",
        "        3. Backward propagation\n",
        "        4. Update parameters\n",
        "        5. Store loss\n",
        "\n",
        "        Must populate self.loss_history!\n",
        "        \"\"\"\n",
        "        self.initialize_parameters()\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            A_L = self.forward_propagation(X)\n",
        "            loss = self.compute_loss(A_L, y)\n",
        "            grads = self.backward_propagation(y)\n",
        "            self.update_parameters(grads)\n",
        "            self.loss_history.append(loss)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Use forward_propagation and apply appropriate thresholding\n",
        "        \"\"\"\n",
        "        prediction = self.forward_propagation(X).flatten()\n",
        "        return (prediction >= threshold).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "foCVrsiTBBug"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training MLP...\n",
            "MLP training completed in 800.40s\n",
            "Loss decreased from 0.6931 to 0.4133\n"
          ]
        }
      ],
      "source": [
        "# Train MLP\n",
        "print(\"Training MLP...\")\n",
        "mlp_start_time = time.time()\n",
        "\n",
        "mlp_architecture = [n_features, 16, 8, 1]  \n",
        "mlp_model = MLP(architecture=mlp_architecture, learning_rate=0.001, n_iterations=10000)\n",
        "mlp_model.fit(X_train_scaled , y_train)\n",
        "\n",
        "mlp_predictions = mlp_model.predict(X_test_scaled)\n",
        "\n",
        "mlp_training_time = time.time() - mlp_start_time\n",
        "print(f\"MLP training completed in {mlp_training_time:.2f}s\")\n",
        "print(f\"Loss decreased from {mlp_model.loss_history[0]:.4f} to {mlp_model.loss_history[-1]:.4f}\")\n",
        "\n",
        "# Store loss explicitly\n",
        "mlp_initial_loss = 0.0 #mlp_model.loss_history[0]\n",
        "mlp_final_loss = 0.0 #mlp_model.loss_history[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf38UvC0BBug"
      },
      "source": [
        "## Section 5: Evaluation and Metrics\n",
        "\n",
        "Calculate appropriate metrics for your problem type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dei5PjKGBBug"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Model Performance:\n",
            "\n",
            "MLP Model Performance:\n"
          ]
        }
      ],
      "source": [
        "def calculate_metrics(y_true, y_pred, problem_type):\n",
        "    \"\"\"\n",
        "    TODO: Calculate appropriate metrics based on problem type\n",
        "\n",
        "    For regression: MSE, RMSE, MAE, R²\n",
        "    For classification: Accuracy, Precision, Recall, F1\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    if problem_type == \"regression\":\n",
        "        # TODO: Calculate regression metrics\n",
        "        # TODO: Implement from scratch\n",
        "        mse = 0.0\n",
        "        rmse = 0.0\n",
        "        mae = 0.0\n",
        "        r2 = 0.0\n",
        "        return mse, rmse, mae, r2\n",
        "        pass\n",
        "    elif problem_type in [\"binary_classification\", \"multiclass_classification\"]:\n",
        "        # TODO: Calculate classification metrics\n",
        "        # TODO: Implement from scratch (no sklearn.metrics)\n",
        "        accuracy = 0.0\n",
        "        precision = 0.0\n",
        "        recall = 0.0\n",
        "        f1 = 0.0\n",
        "        return accuracy, precision, recall, f1\n",
        "        pass\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for both models\n",
        "# baseline_metrics = calculate_metrics(y_test, baseline_predictions, problem_type)\n",
        "# mlp_metrics = calculate_metrics(y_test, mlp_predictions, problem_type)\n",
        "\n",
        "print(\"Baseline Model Performance:\")\n",
        "# print(baseline_metrics)\n",
        "\n",
        "print(\"\\nMLP Model Performance:\")\n",
        "# print(mlp_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4SZCkf3BBug"
      },
      "source": [
        "## Section 6: Visualization\n",
        "\n",
        "Create visualizations:\n",
        "1. Training loss curves\n",
        "2. Performance comparison\n",
        "3. Additional domain-specific plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPqji5F0BBuh"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 1. Training loss curves\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# TODO: Plot baseline loss\n",
        "# plt.plot(baseline_model.loss_history, label='Baseline', color='blue')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Baseline Model - Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# TODO: Plot MLP loss\n",
        "# plt.plot(mlp_model.loss_history, label='MLP', color='red')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('MLP Model - Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Pj6pQZBBuh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2. Performance comparison bar chart\n",
        "# TODO: Create bar chart comparing key metrics between models\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Example:\n",
        "# metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "# baseline_scores = [baseline_metrics[m] for m in metrics]\n",
        "# mlp_scores = [mlp_metrics[m] for m in metrics]\n",
        "#\n",
        "# x = np.arange(len(metrics))\n",
        "# width = 0.35\n",
        "#\n",
        "# plt.bar(x - width/2, baseline_scores, width, label='Baseline')\n",
        "# plt.bar(x + width/2, mlp_scores, width, label='MLP')\n",
        "# plt.xlabel('Metrics')\n",
        "# plt.ylabel('Score')\n",
        "# plt.title('Model Performance Comparison')\n",
        "# plt.xticks(x, metrics)\n",
        "# plt.legend()\n",
        "# plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua107rhpBBuh"
      },
      "source": [
        "## Section 7: Analysis and Discussion\n",
        "\n",
        "Write your analysis (minimum 200 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fdNLJqCBBuh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis word count: 82 words\n",
            "⚠️  Warning: Analysis should be at least 200 words\n"
          ]
        }
      ],
      "source": [
        "analysis_text = \"\"\"\n",
        "TODO: Write your analysis here (minimum 200 words)\n",
        "\n",
        "Address these questions:\n",
        "1. Which model performed better and by how much?\n",
        "2. Why do you think one model outperformed the other?\n",
        "3. What was the computational cost difference (training time)?\n",
        "4. Any surprising findings or challenges you faced?\n",
        "5. What insights did you gain about neural networks vs linear models?\n",
        "\n",
        "Write your thoughtful analysis here. Be specific and reference your actual results.\n",
        "Compare the metrics, discuss the trade-offs, and explain what you learned.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
        "if len(analysis_text.split()) < 200:\n",
        "    print(\"⚠️  Warning: Analysis should be at least 200 words\")\n",
        "else:\n",
        "    print(\"✓ Analysis meets word count requirement\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e3C2Bf4BBuh"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## ⭐ REQUIRED: Structured Output Function\n",
        "\n",
        "### **DO NOT MODIFY THE STRUCTURE BELOW**\n",
        "\n",
        "This function will be called by the auto-grader. Fill in all values accurately based on your actual results.\n",
        "\n",
        "\n",
        "⭐⭐⭐ REQUIRED: Structured Output Function ⭐⭐⭐\n",
        "\n",
        "### 🚨 CRITICAL - READ CAREFULLY 🚨\n",
        "\n",
        "1. **Fill in ALL fields** - Missing fields = 0 marks\n",
        "2. **Use your actual values** - Not 0 or empty strings\n",
        "3. **This cell MUST be executed** - We need the output!\n",
        "4. **Print the results** - Auto-grader needs to see output!\n",
        "\n",
        "\n",
        "**DO NOT:**\n",
        "- Leave any field as 0, 0.0,\n",
        "- Clear outputs before submission\n",
        "- Modify the structure\n",
        "\n",
        "\n",
        "\"**MUST DO:**\n",
        "- Fill every field with your actual results\n",
        "- Execute this cell and keep the output\n",
        "- Print the results (see below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1W6qvYzBBuh"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'baseline_acc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ===== CRITICAL: CALL AND PRINT RESULTS =====\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# This MUST be executed and output MUST be visible!\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_assignment_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(results, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# ===== Validation =====\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[14], line 34\u001b[0m, in \u001b[0;36mget_assignment_results\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_assignment_results\u001b[39m():\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    CRITICAL: Fill ALL fields with your actual results!\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Missing fields will result in 0 marks for that section.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# ===== Dataset Information (1 mark) =====\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_name,  \u001b[38;5;66;03m# MUST fill\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_source\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_source,  \u001b[38;5;66;03m# MUST fill\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: n_samples,  \u001b[38;5;66;03m# MUST be ≥500\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_features\u001b[39m\u001b[38;5;124m'\u001b[39m: n_features,  \u001b[38;5;66;03m# MUST be ≥5\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem_type\u001b[39m\u001b[38;5;124m'\u001b[39m: problem_type,  \u001b[38;5;66;03m# MUST fill\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem_statement\u001b[39m\u001b[38;5;124m'\u001b[39m: problem_statement,  \u001b[38;5;66;03m# MUST be ≥50 words\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: primary_metric,  \u001b[38;5;66;03m# MUST fill\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_justification\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_justification,  \u001b[38;5;66;03m# MUST be ≥30 words\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: train_samples,\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: test_samples,\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_test_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: train_test_ratio,\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# ===== Baseline Model (3 marks) =====\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline_model\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m     23\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# 'linear_regression', 'logistic_regression', 'softmax_regression'\u001b[39;00m\n\u001b[0;32m     24\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,  \u001b[38;5;66;03m# Your learning rate\u001b[39;00m\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m,  \u001b[38;5;66;03m# Your iterations\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m             \u001b[38;5;66;03m# CRITICAL: These MUST be filled!\u001b[39;00m\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline_initial_loss,  \u001b[38;5;66;03m# MUST NOT be 0\u001b[39;00m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline_final_loss,  \u001b[38;5;66;03m# MUST NOT be 0\u001b[39;00m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time_seconds\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline_training_time,  \u001b[38;5;66;03m# MUST NOT be 0\u001b[39;00m\n\u001b[0;32m     31\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_decreased\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline_final_loss \u001b[38;5;241m<\u001b[39m baseline_initial_loss,  \u001b[38;5;66;03m# Auto-calculated\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m             \u001b[38;5;66;03m# Metrics - Fill based on your problem type\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mbaseline_acc\u001b[49m,\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_precision\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m baseline_prec,\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_recall\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m baseline_rec,\n\u001b[0;32m     37\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m baseline_f1,\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_mse\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline_mse \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_rmse\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline_rmse \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_mae\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline_mae \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     41\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_r2\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline_r2 \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     42\u001b[0m         },\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# ===== MLP Model (4 marks) =====\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp_model\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_architecture,  \u001b[38;5;66;03m# MUST have ≥3 elements\u001b[39;00m\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_hidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(mlp_architecture) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mlp_architecture) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m             \u001b[38;5;66;03m# CRITICAL: These MUST be filled!\u001b[39;00m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_initial_loss,  \u001b[38;5;66;03m# MUST NOT be 0\u001b[39;00m\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_final_loss,  \u001b[38;5;66;03m# MUST NOT be 0\u001b[39;00m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time_seconds\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_training_time,  \u001b[38;5;66;03m# MUST NOT be 0\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_decreased\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_final_loss \u001b[38;5;241m<\u001b[39m mlp_initial_loss,  \u001b[38;5;66;03m# Auto-calculated\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m             \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[0;32m     58\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m mlp_acc,\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_precision\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m mlp_prec,\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_recall\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m mlp_rec,\n\u001b[0;32m     61\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m mlp_f1,\n\u001b[0;32m     62\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_mse\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_mse \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_rmse\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_rmse \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_mae\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_mae \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     65\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_r2\u001b[39m\u001b[38;5;124m'\u001b[39m: mlp_r2 \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     66\u001b[0m         },\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# ===== Analysis (2 marks) =====\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m'\u001b[39m: analysis_text,\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis_word_count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(analysis_text\u001b[38;5;241m.\u001b[39msplit()),\n\u001b[0;32m     71\u001b[0m     }\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
            "\u001b[1;31mNameError\u001b[0m: name 'baseline_acc' is not defined"
          ]
        }
      ],
      "source": [
        "def get_assignment_results():\n",
        "    '''\n",
        "    CRITICAL: Fill ALL fields with your actual results!\n",
        "    Missing fields will result in 0 marks for that section.\n",
        "    '''\n",
        "\n",
        "    results = {\n",
        "        # ===== Dataset Information (1 mark) =====\n",
        "        'dataset_name': dataset_name,  # MUST fill\n",
        "        'dataset_source': dataset_source,  # MUST fill\n",
        "        'n_samples': n_samples,  # MUST be ≥500\n",
        "        'n_features': n_features,  # MUST be ≥5\n",
        "        'problem_type': problem_type,  # MUST fill\n",
        "        'problem_statement': problem_statement,  # MUST be ≥50 words\n",
        "        'primary_metric': primary_metric,  # MUST fill\n",
        "        'metric_justification': metric_justification,  # MUST be ≥30 words\n",
        "        'train_samples': train_samples,\n",
        "        'test_samples': test_samples,\n",
        "        'train_test_ratio': train_test_ratio,\n",
        "\n",
        "        # ===== Baseline Model (3 marks) =====\n",
        "        'baseline_model': {\n",
        "            'model_type': '',  # 'linear_regression', 'logistic_regression', 'softmax_regression'\n",
        "            'learning_rate': 0.01,  # Your learning rate\n",
        "            'n_iterations': 1000,  # Your iterations\n",
        "\n",
        "            # CRITICAL: These MUST be filled!\n",
        "            'initial_loss': baseline_initial_loss,  # MUST NOT be 0\n",
        "            'final_loss': baseline_final_loss,  # MUST NOT be 0\n",
        "            'training_time_seconds': baseline_training_time,  # MUST NOT be 0\n",
        "            'loss_decreased': baseline_final_loss < baseline_initial_loss,  # Auto-calculated\n",
        "\n",
        "            # Metrics - Fill based on your problem type\n",
        "            'test_accuracy': 0.0 if problem_type == 'regression' else baseline_acc,\n",
        "            'test_precision': 0.0 if problem_type == 'regression' else baseline_prec,\n",
        "            'test_recall': 0.0 if problem_type == 'regression' else baseline_rec,\n",
        "            'test_f1': 0.0 if problem_type == 'regression' else baseline_f1,\n",
        "            'test_mse': baseline_mse if problem_type == 'regression' else 0.0,\n",
        "            'test_rmse': baseline_rmse if problem_type == 'regression' else 0.0,\n",
        "            'test_mae': baseline_mae if problem_type == 'regression' else 0.0,\n",
        "            'test_r2': baseline_r2 if problem_type == 'regression' else 0.0,\n",
        "        },\n",
        "\n",
        "        # ===== MLP Model (4 marks) =====\n",
        "        'mlp_model': {\n",
        "            'architecture': mlp_architecture,  # MUST have ≥3 elements\n",
        "            'n_hidden_layers': len(mlp_architecture) - 2 if len(mlp_architecture) > 0 else 0,\n",
        "            'learning_rate': 0.01,\n",
        "            'n_iterations': 1000,\n",
        "\n",
        "            # CRITICAL: These MUST be filled!\n",
        "            'initial_loss': mlp_initial_loss,  # MUST NOT be 0\n",
        "            'final_loss': mlp_final_loss,  # MUST NOT be 0\n",
        "            'training_time_seconds': mlp_training_time,  # MUST NOT be 0\n",
        "            'loss_decreased': mlp_final_loss < mlp_initial_loss,  # Auto-calculated\n",
        "\n",
        "            # Metrics\n",
        "            'test_accuracy': 0.0 if problem_type == 'regression' else mlp_acc,\n",
        "            'test_precision': 0.0 if problem_type == 'regression' else mlp_prec,\n",
        "            'test_recall': 0.0 if problem_type == 'regression' else mlp_rec,\n",
        "            'test_f1': 0.0 if problem_type == 'regression' else mlp_f1,\n",
        "            'test_mse': mlp_mse if problem_type == 'regression' else 0.0,\n",
        "            'test_rmse': mlp_rmse if problem_type == 'regression' else 0.0,\n",
        "            'test_mae': mlp_mae if problem_type == 'regression' else 0.0,\n",
        "            'test_r2': mlp_r2 if problem_type == 'regression' else 0.0,\n",
        "        },\n",
        "\n",
        "        # ===== Analysis (2 marks) =====\n",
        "        'analysis': analysis_text,\n",
        "        'analysis_word_count': len(analysis_text.split()),\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# ===== CRITICAL: CALL AND PRINT RESULTS =====\n",
        "# This MUST be executed and output MUST be visible!\n",
        "import json\n",
        "results = get_assignment_results()\n",
        "print(json.dumps(results, indent=2))\n",
        "\n",
        "# ===== Validation =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "errors = []\n",
        "\n",
        "if results['n_samples'] < 500:\n",
        "    errors.append(f\"❌ Dataset too small: {results['n_samples']} < 500\")\n",
        "if results['n_features'] < 5:\n",
        "    errors.append(f\"❌ Too few features: {results['n_features']} < 5\")\n",
        "if results['baseline_model']['initial_loss'] == 0:\n",
        "    errors.append(\"❌ Baseline initial_loss is 0\")\n",
        "if results['baseline_model']['final_loss'] == 0:\n",
        "    errors.append(\"❌ Baseline final_loss is 0\")\n",
        "if results['baseline_model']['training_time_seconds'] == 0:\n",
        "    errors.append(\"❌ Baseline training_time is 0\")\n",
        "if results['mlp_model']['initial_loss'] == 0:\n",
        "    errors.append(\"❌ MLP initial_loss is 0\")\n",
        "if results['mlp_model']['final_loss'] == 0:\n",
        "    errors.append(\"❌ MLP final_loss is 0\")\n",
        "if results['mlp_model']['training_time_seconds'] == 0:\n",
        "    errors.append(\"❌ MLP training_time is 0\")\n",
        "if len(results['mlp_model']['architecture']) < 3:\n",
        "    errors.append(\"❌ MLP architecture invalid\")\n",
        "if results['analysis_word_count'] < 200:\n",
        "    errors.append(f\"❌ Analysis too short: {results['analysis_word_count']} < 200 words\")\n",
        "\n",
        "if errors:\n",
        "    print(\"ERRORS FOUND:\")\n",
        "    for err in errors:\n",
        "        print(err)\n",
        "    print(\" FIX THESE BEFORE SUBMITTING! \")\n",
        "else:\n",
        "    print(\"✅ All validation checks passed!\")\n",
        "    print(\"✅ Ready to submit!\")\n",
        "    print(\"Next steps:\")\n",
        "    print(\"1. Kernel → Restart & Clear Output\")\n",
        "    print(\"2. Kernel → Restart & Run All\")\n",
        "    print(\"3. Verify this output is visible\")\n",
        "    print(\"4. Save notebook\")\n",
        "    print(\"5. Rename as: YourStudentID_assignment.ipynb\")\n",
        "    print(\"6. Submit to LMS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPvwotBWBBuh"
      },
      "source": [
        "## Test Your Output\n",
        "\n",
        "Run this cell to verify your results dictionary is complete and properly formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4mkH-nSBBui"
      },
      "outputs": [],
      "source": [
        "# Test the output\n",
        "import json\n",
        "\n",
        "try:\n",
        "    results = get_assignment_results()\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(json.dumps(results, indent=2))\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "\n",
        "    # Check for missing values\n",
        "    missing = []\n",
        "    def check_dict(d, prefix=\"\"):\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, dict):\n",
        "                check_dict(v, f\"{prefix}{k}.\")\n",
        "            elif (v == 0 or v == \"\" or v == 0.0 or v == []) and \\\n",
        "                 k not in ['improvement', 'improvement_percentage', 'baseline_better',\n",
        "                          'baseline_converged', 'mlp_converged', 'total_parameters',\n",
        "                          'test_accuracy', 'test_precision', 'test_recall', 'test_f1',\n",
        "                          'test_mse', 'test_rmse', 'test_mae', 'test_r2']:\n",
        "                missing.append(f\"{prefix}{k}\")\n",
        "\n",
        "    check_dict(results)\n",
        "\n",
        "    if missing:\n",
        "        print(f\"⚠️  Warning: {len(missing)} fields still need to be filled:\")\n",
        "        for m in missing[:15]:  # Show first 15\n",
        "            print(f\"  - {m}\")\n",
        "        if len(missing) > 15:\n",
        "            print(f\"  ... and {len(missing)-15} more\")\n",
        "    else:\n",
        "        print(\"✅ All required fields are filled!\")\n",
        "        print(\"\\n🎉 You're ready to submit!\")\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Kernel → Restart & Clear Output\")\n",
        "        print(\"2. Kernel → Restart & Run All\")\n",
        "        print(\"3. Verify no errors\")\n",
        "        print(\"4. Save notebook\")\n",
        "        print(\"5. Rename as: YourStudentID_assignment.ipynb\")\n",
        "        print(\"6. Submit to LMS\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error in get_assignment_results(): {str(e)}\")\n",
        "    print(\"\\nPlease fix the errors above before submitting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuRKuL80BBui"
      },
      "source": [
        "---\n",
        "\n",
        "## 📤 Before Submitting - Final Checklist\n",
        "\n",
        "- [ ] **All TODO sections completed**\n",
        "- [ ] **Both models implemented from scratch** (no sklearn models!)\n",
        "- [ ] **get_assignment_results() function filled accurately**\n",
        "- [ ] **Loss decreases for both models**\n",
        "- [ ] **Analysis ≥ 200 words**\n",
        "- [ ] **All cells run without errors** (Restart & Run All)\n",
        "- [ ] **Visualizations created**\n",
        "- [ ] **File renamed correctly**: YourStudentID_assignment.ipynb\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck! **"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
