{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srini235/DNN_Assignment_01_Group250-/blob/main/Group250%20assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64-XCLdSBBuc"
      },
      "source": [
        "# Deep Neural Networks - Programming Assignment\n",
        "## Comparing Linear Models and Multi-Layer Perceptrons\n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "\n",
        "**Date:** ___________________\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ IMPORTANT INSTRUCTIONS\n",
        "\n",
        "1. **Complete ALL sections** marked with `TODO`\n",
        "2. **DO NOT modify** the `get_assignment_results()` function structure\n",
        "3. **Track training time** for both models using `time.time()`\\n\n",
        "4. **Store loss_history** in both model classes\n",
        "5. **Calculate ALL metrics** (accuracy, precision, recall, F1)\n",
        "6. **Fill get_assignment_results()** with ALL required fields\n",
        "7. **PRINT the results** - Auto-grader needs visible output!\n",
        "8. **Run all cells** before submitting (Kernel â†’ Restart & Run All)\n",
        "\n",
        "**SCORING:**\n",
        "- Missing fields = 0 marks for that section\n",
        "- Non-executed notebook = 0 marks\n",
        "- Cleared outputs = 0 marks\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (2.2.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (3.10.8)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: pyparsing>=3 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: pillow>=8 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: numpy>=1.23 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: six>=1.5 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (1.7.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.22.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ucimlrepo in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from ucimlrepo) (2.3.3)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from ucimlrepo) (2025.11.12)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in d:\\06_higherstudies\\01_ai_ml\\subjects\\02_semester02\\deep neural networks\\assignment\\assignment_1\\dnn_assignment_01_group250-\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'D:\\06_HigherStudies\\01_AI_ML\\Subjects\\02_Semester02\\Deep Neural Networks\\Assignment\\Assignment_1\\DNN_Assignment_01_Group250-\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3Pti5nJuBBue"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import datasets\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "print('Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNyOCAuwBBue"
      },
      "source": [
        "## Section 1: Dataset Selection and Loading\n",
        "\n",
        "**Requirements:**\n",
        "- â‰¥500 samples\n",
        "- â‰¥5 features\n",
        "- Public dataset (UCI/Kaggle)\n",
        "- Regression OR Classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Load your dataset\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo \n",
        "  \n",
        "# # fetch dataset \n",
        "# cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
        "  \n",
        "# # data (as pandas dataframes) \n",
        "# x = cdc_diabetes_health_indicators.data.features \n",
        "# y = cdc_diabetes_health_indicators.data.targets \n",
        "  \n",
        "# # metadata \n",
        "# print(cdc_diabetes_health_indicators.metadata) \n",
        "  \n",
        "# # variable information \n",
        "# print(cdc_diabetes_health_indicators.variables) \n",
        "\n",
        "x, y = datasets.load_wine(return_X_y=True, as_frame=True)\n",
        "x = x.to_numpy()\n",
        "y = y.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "9QfsHoIkBBuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: CDC Diabetes Health Indicators\n",
            "Source: UCI ML Repository\n",
            "Samples: 253680, Features: 13\n",
            "Problem Type: multiclass_classification\n",
            "Primary Metric: recall\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load your dataset\n",
        "# Example: data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Dataset information (TODO: Fill these)\n",
        "dataset_name = \"CDC Diabetes Health Indicators\"  # e.g., \"Breast Cancer Wisconsin\"\n",
        "dataset_source = \"UCI ML Repository\"  # e.g., \"UCI ML Repository\"\n",
        "n_samples = 253680      # Total number of rows\n",
        "n_features = x.shape[1]     # Number of features (excluding target)\n",
        "problem_type = \"multiclass_classification\"  # \"regression\" or \"binary_classification\" or \"multiclass_classification\"\n",
        "\n",
        "# Problem statement (TODO: Write 2-3 sentences)\n",
        "problem_statement = \"\"\"\n",
        "Predicting diabetes health indicators from patient data.\n",
        "This is critical for early intervention and management of diabetes in healthcare settings.\n",
        "\"\"\"\n",
        "\n",
        "# Primary evaluation metric (TODO: Fill this)\n",
        "primary_metric = \"recall\"  # e.g., \"recall\", \"accuracy\", \"rmse\", \"r2\"\n",
        "# Metric justification (TODO: Write 2-3 sentences)\n",
        "metric_justification = \"\"\"\n",
        "I chose recall because in medical diagnosis,\n",
        "false negatives (missing diabetes cases) are more costly than false positives.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Source: {dataset_source}\")\n",
        "print(f\"Samples: {n_samples}, Features: {n_features}\")\n",
        "print(f\"Problem Type: {problem_type}\")\n",
        "print(f\"Primary Metric: {primary_metric}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XOI6I6JBBuf"
      },
      "source": [
        "## Section 2: Data Preprocessing\n",
        "\n",
        "Preprocess your data:\n",
        "1. Handle missing values\n",
        "2. Encode categorical variables\n",
        "3. Split into train/test sets\n",
        "4. Scale features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "mfQProeUBBuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 142\n",
            "Test samples: 36\n",
            "Split ratio: 80.0%\n"
          ]
        }
      ],
      "source": [
        "# TODO: Preprocess your data\n",
        "# 1. Separate features (X) and target (y)\n",
        "# 2. Handle missing values if any\n",
        "# 3. Encode categorical variables\n",
        "\n",
        "# Example:\n",
        "# X = data.drop('target', axis=1)\n",
        "# y = data['target']\n",
        "\n",
        "# TODO: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fill these after preprocessing\n",
        "train_samples = len(y_train)       # Number of training samples\n",
        "test_samples = len(y_test)        # Number of test samples\n",
        "train_test_ratio = 0.8  # e.g., 0.8 for 80-20 split\n",
        "\n",
        "print(f\"Train samples: {train_samples}\")\n",
        "print(f\"Test samples: {test_samples}\")\n",
        "print(f\"Split ratio: {train_test_ratio:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elw6-stdBBuf"
      },
      "source": [
        "## Section 3: Baseline Model Implementation\n",
        "\n",
        "Implement from scratch (NO sklearn models!):\n",
        "- Linear Regression (for regression)\n",
        "- Logistic Regression (for binary classification)\n",
        "- Softmax Regression (for multiclass classification)\n",
        "\n",
        "**Must include:**\n",
        "- Forward pass (prediction)\n",
        "- Loss computation\n",
        "- Gradient computation\n",
        "- Gradient descent loop\n",
        "- Loss tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvQ2rWfWBBuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Baseline model class defined\n"
          ]
        }
      ],
      "source": [
        "class BaselineModel:\n",
        "    \"\"\"\n",
        "    Baseline linear model with gradient descent\n",
        "    Implement: Linear/Logistic/Softmax Regression\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.05, n_iterations=1000, batch_size=32):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size # New parameter for mini-batch\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        # Automatically determine number of classes for multiclass classification\n",
        "        if problem_type == \"multiclass_classification\":\n",
        "            self.n_classes = len(np.unique(y))\n",
        "\n",
        "    def xavier_init(self, n_features, n_classes):\n",
        "        \"\"\"Xavier/Glorot initialization for weights\"\"\"\n",
        "        limit = np.sqrt(6 / (n_features + n_classes))\n",
        "        return np.random.uniform(-limit, limit, (n_features, n_classes))\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"\n",
        "        Compute softmax probabilities for multiclass classification\"\"\"\n",
        "        logits = np.dot(X, self.weights) + self.bias\n",
        "        exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        y_pred = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "        return y_pred\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        TODO: Implement gradient descent training\n",
        "\n",
        "        Steps:\n",
        "        1. Initialize weights and bias\n",
        "        2. For each iteration:\n",
        "           a. Compute predictions (forward pass)\n",
        "           b. Compute loss\n",
        "           c. Compute gradients\n",
        "           d. Update weights and bias\n",
        "           e. Store loss in self.loss_history\n",
        "\n",
        "        Must populate self.loss_history with loss at each iteration!\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Xavier/Glorot initialization for multiclass classification\n",
        "        if problem_type == \"multiclass_classification\":\n",
        "            #self.weights = self.xavier_init(n_features, self.n_classes)\n",
        "            self.weights = np.zeros((n_features, self.n_classes))\n",
        "        else:\n",
        "            self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        y_one_hot = np.eye(self.n_classes)[y.astype(int)] if problem_type == \"multiclass_classification\" else y\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # TODO: Implement gradient descent loop\n",
        "        # Mini batch gradient descent\n",
        "        for epoch in range(self.n_iterations // self.batch_size):\n",
        "            # Shuffle the data at the beginning of each epoch\n",
        "            indices = np.arange(n_samples)\n",
        "            np.random.shuffle(indices)\n",
        "            X_shuffled = X[indices]\n",
        "            y_one_hot_shuffled = y_one_hot[indices]\n",
        "            # Mini-batch processing\n",
        "            for epoch_iter in range(0, n_samples, self.batch_size):\n",
        "                end_idx = min(epoch_iter + self.batch_size, n_samples)\n",
        "                X_batch = X_shuffled[epoch_iter:end_idx]\n",
        "                y_one_hot_batch = y_one_hot_shuffled[epoch_iter:end_idx]\n",
        "                current_batch_size = X_batch.shape[0]\n",
        "\n",
        "                if current_batch_size == 0:\n",
        "                    continue  # Skip empty batch\n",
        "\n",
        "                # 1. Forward pass: (for multiclass classification, use softmax)\n",
        "\n",
        "                # Compute logits (linear output)\n",
        "                logits = np.dot(X_batch, self.weights) + self.bias\n",
        "                # Softmax for multiclass classification\n",
        "                # Find exponential scores\n",
        "                exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "                # Softmax probabilities\n",
        "                y_pred = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "                # 2. Compute categorical cross-entropy loss\n",
        "                # Clip probabilities to avoid log(0) for numerical stability\n",
        "                y_pred_clipped = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
        "                loss = -np.sum(y_one_hot_batch * np.log(y_pred_clipped)) / current_batch_size\n",
        "                epoch_loss += loss\n",
        "                num_batches += 1\n",
        "\n",
        "                # 3. Compute gradients: dw = ..., db = ...\n",
        "                dw = (1 / current_batch_size) * np.dot(X_batch.T, (y_pred_clipped - y_one_hot_batch))\n",
        "                db = (1 / current_batch_size) * np.sum(y_pred_clipped - y_one_hot_batch)\n",
        "                # 4. Update: self.weights -= self.lr * dw\n",
        "                self.weights -= self.lr * dw\n",
        "                self.bias -= self.lr * db\n",
        "                # 5. Append loss to history\n",
        "                self.loss_history.append(loss)\n",
        "            if num_batches > 0:\n",
        "                self.loss_history.append(epoch_loss / num_batches) # Store average loss for the epoch\n",
        "            else:\n",
        "                self.loss_history.append(epoch_loss) # Should only happen if n_samples=0\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Implement prediction\n",
        "\n",
        "        For regression: return linear_output\n",
        "        For classification: return class probabilities or labels\n",
        "        \"\"\"\n",
        "        if self.weights is None or self.bias is None:\n",
        "            raise RuntimeError(\"Model has not been trained yet. Call .fit() first.\")\n",
        "\n",
        "        # Compute logits (linear output)\n",
        "        logits = np.dot(X, self.weights) + self.bias\n",
        "        # Softmax for multiclass classification\n",
        "        # Find exponential scores\n",
        "        exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        # Softmax probabilities\n",
        "        y_pred = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        #probabilities = self._softmax(logits)\n",
        "        return np.argmax(y_pred, axis=1) # Return the class index with the highest probability\n",
        "\n",
        "print(\"âœ“ Baseline model class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "grpLqkmTBBuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training baseline model...\n",
            "âœ“ Baseline training completed in 0.00s\n",
            "âœ“ Loss decreased from 1.0986 to 0.2413\n"
          ]
        }
      ],
      "source": [
        "# Train baseline model\n",
        "print(\"Training baseline model...\")\n",
        "baseline_start = time.time()\n",
        "\n",
        "# TODO: Initialize and train your baseline model\n",
        "baseline_model = BaselineModel(learning_rate=0.1, n_iterations=2000, batch_size=64)\n",
        "baseline_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# TODO: Make predictions\n",
        "baseline_predictions = baseline_model.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "\n",
        "baseline_training_time = time.time() - baseline_start\n",
        "print(f\"âœ“ Baseline training completed in {baseline_training_time:.2f}s\")\n",
        "print(f\"âœ“ Loss decreased from {baseline_model.loss_history[0]:.4f} to {baseline_model.loss_history[-1]:.4f}\")\n",
        "\n",
        "# Store loss explicitly\n",
        "baseline_initial_loss = baseline_model.loss_history[0]\n",
        "baseline_final_loss = baseline_model.loss_history[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQpy-zZSBBug"
      },
      "source": [
        "## Section 4: Multi-Layer Perceptron Implementation\n",
        "\n",
        "Implement MLP from scratch with:\n",
        "- At least 1 hidden layer\n",
        "- ReLU activation for hidden layers\n",
        "- Appropriate output activation\n",
        "- Forward propagation\n",
        "- Backward propagation\n",
        "- Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "r2wqZVlfBBug"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron implemented from scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, architecture, learning_rate=0.01, n_iterations=1000):\n",
        "        \"\"\"\n",
        "        architecture: list [input_size, hidden1, hidden2, ..., output_size]\n",
        "        Example: [30, 16, 8, 1] means:\n",
        "            - 30 input features\n",
        "            - Hidden layer 1: 16 neurons\n",
        "            - Hidden layer 2: 8 neurons\n",
        "            - Output layer: 1 neuron\n",
        "        \"\"\"\n",
        "        self.architecture = architecture\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.parameters = {}\n",
        "        self.loss_history = []\n",
        "        self.cache = {}\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        TODO: Initialize weights and biases for all layers\n",
        "\n",
        "        For each layer l:\n",
        "        - W[layer_index]: weight matrix of shape (n[layer_index], n[layer_index-1])\n",
        "        - b[layer_index]: bias vector of shape (n[layer_index], 1)\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "\n",
        "        for layer_index in range(1, len(self.architecture)):\n",
        "            num_perceptrons_current = self.architecture[layer_index]\n",
        "            num_perceptrons_previous = self.architecture[layer_index-1]\n",
        "            # Weights\n",
        "            # # He init for hidden layers; Xavier-ish for output\n",
        "            # if layer_index < len(self.architecture) - 1:\n",
        "            #     self.parameters[f'W{layer_index}'] = np.random.randn(num_perceptrons_current, num_perceptrons_previous) \\\n",
        "            #         * np.sqrt(2/num_perceptrons_previous)\n",
        "            # else:\n",
        "            #     self.parameters[f'W{layer_index}'] = np.random.randn(num_perceptrons_current, num_perceptrons_previous) \\\n",
        "            #         * np.sqrt(1/num_perceptrons_previous)\n",
        "            self.parameters[f'W{layer_index}'] = np.zeros((num_perceptrons_current, num_perceptrons_previous))\n",
        "            # print(f'W{layer_index}: {self.parameters[f'W{layer_index}'].shape}')\n",
        "\n",
        "            # Bias\n",
        "            self.parameters[f'b{layer_index}'] = np.zeros((num_perceptrons_current, 1))\n",
        "            # print(f'b{layer_index}: {self.parameters[f'b{layer_index}'].shape}')\n",
        "\n",
        "\n",
        "\n",
        "    def relu(self, Z):\n",
        "        \"\"\"ReLU activation function\"\"\"\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "        \"\"\"ReLU derivative\"\"\"\n",
        "        return (Z > 0).astype(float)\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        \"\"\"Sigmoid activation (for binary classification output)\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Implement forward pass through all layers\n",
        "\n",
        "        For each layer:\n",
        "        1. Z[l] = W[l] @ A[l-1] + b[l]\n",
        "        2. A[l] = activation(Z[l])\n",
        "\n",
        "        Store Z and A in self.cache for backpropagation\n",
        "        Return final activation A[L]\n",
        "        \"\"\"\n",
        "        fc_in = X.T\n",
        "        self.cache['A0'] = fc_in\n",
        "        num_layers = len(self.architecture) - 1\n",
        "        for layer_index in range(1, num_layers + 1):\n",
        "            weight, bias = self.parameters[f'W{layer_index}'], self.parameters[f'b{layer_index}']\n",
        "            fc_out = np.dot(weight, fc_in) + bias\n",
        "            activation = self.relu(fc_out) if layer_index < num_layers else self.sigmoid(fc_out)\n",
        "            self.cache[f'Z{layer_index}'] = fc_out\n",
        "            self.cache[f'A{layer_index}'] = activation\n",
        "            fc_in = activation\n",
        "        \n",
        "        final_layer_activation = activation\n",
        "        return final_layer_activation\n",
        "\n",
        "        # TODO: Implement forward pass\n",
        "        # for l in range(1, len(self.architecture)):\n",
        "        #     ...\n",
        "\n",
        "    def backward_propagation(self, y):\n",
        "        grads = {}\n",
        "        num_samples = y.shape[0]\n",
        "        num_layers = len(self.architecture) - 1\n",
        "        activation = self.cache[f'A{num_layers}'] \n",
        "        previous_layer_activation = self.cache[f'A{num_layers-1}'] if num_layers-1 >= 0 else self.cache['A0']\n",
        "        dZ_L = (activation - y.reshape(1, num_samples))         \n",
        "        grads[f'dW{num_layers}'] = (1/num_samples) * np.dot(dZ_L, previous_layer_activation.T)\n",
        "        grads[f'db{num_layers}'] = (1/num_samples) * np.sum(dZ_L, axis=1, keepdims=True)\n",
        "\n",
        "        for layer_index in range(num_layers-1, 0, -1):\n",
        "            W_next = self.parameters[f'W{layer_index+1}']\n",
        "            dZ_next = dZ_L if layer_index == num_layers-1 else grads[f'dZ{layer_index+1}']\n",
        "            previous_layer_activation = self.cache[f'A{layer_index-1}'] if layer_index-1 >= 0 else self.cache['A0']\n",
        "            Z_l = self.cache[f'Z{layer_index}']\n",
        "            dA_l = np.dot(W_next.T, dZ_next)\n",
        "            dZ_l = dA_l * self.relu_derivative(Z_l)\n",
        "            grads[f'dZ{layer_index}'] = dZ_l\n",
        "            grads[f'dW{layer_index}'] = (1/num_samples) * np.dot(dZ_l, previous_layer_activation.T)\n",
        "            grads[f'db{layer_index}'] = (1/num_samples) * np.sum(dZ_l, axis=1, keepdims=True)\n",
        "        return grads\n",
        "\n",
        "    def update_parameters(self, grads):\n",
        "        for layer_index in range(1, len(self.architecture)):\n",
        "            self.parameters[f'W{layer_index}'] -= self.lr * grads[f'dW{layer_index}']\n",
        "            self.parameters[f'b{layer_index}'] -= self.lr * grads[f'db{layer_index}']\n",
        "\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "       m = y_true.shape[0]\n",
        "       eps = 1e-12\n",
        "       y_pred = np.clip(y_pred.flatten(), eps, 1-eps)\n",
        "       return -(1/m) * np.sum(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        For each iteration:\n",
        "        1. Forward propagation\n",
        "        2. Compute loss\n",
        "        3. Backward propagation\n",
        "        4. Update parameters\n",
        "        5. Store loss\n",
        "\n",
        "        Must populate self.loss_history!\n",
        "        \"\"\"\n",
        "        self.initialize_parameters()\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            A_L = self.forward_propagation(X)\n",
        "            loss = self.compute_loss(A_L, y)\n",
        "            grads = self.backward_propagation(y)\n",
        "            self.update_parameters(grads)\n",
        "            self.loss_history.append(loss)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Use forward_propagation and apply appropriate thresholding\n",
        "        \"\"\"\n",
        "        prediction = self.forward_propagation(X).flatten()\n",
        "        return (prediction >= threshold).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "foCVrsiTBBug"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training MLP...\n",
            "MLP training completed in 0.64s\n",
            "Loss decreased from 0.6931 to 0.1990\n"
          ]
        }
      ],
      "source": [
        "# Train MLP\n",
        "print(\"Training MLP...\")\n",
        "mlp_start_time = time.time()\n",
        "\n",
        "mlp_architecture = [n_features, 16, 8, 1]  \n",
        "mlp_model = MLP(architecture=mlp_architecture, learning_rate=0.001, n_iterations=10000)\n",
        "mlp_model.fit(X_train_scaled , y_train)\n",
        "\n",
        "mlp_predictions = mlp_model.predict(X_test_scaled)\n",
        "\n",
        "mlp_training_time = time.time() - mlp_start_time\n",
        "print(f\"MLP training completed in {mlp_training_time:.2f}s\")\n",
        "print(f\"Loss decreased from {mlp_model.loss_history[0]:.4f} to {mlp_model.loss_history[-1]:.4f}\")\n",
        "\n",
        "# Store loss explicitly\n",
        "mlp_initial_loss = 0.0 #mlp_model.loss_history[0]\n",
        "mlp_final_loss = 0.0 #mlp_model.loss_history[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf38UvC0BBug"
      },
      "source": [
        "## Section 5: Evaluation and Metrics\n",
        "\n",
        "Calculate appropriate metrics for your problem type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dei5PjKGBBug"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred, problem_type):\n",
        "    \"\"\"\n",
        "    TODO: Calculate appropriate metrics based on problem type\n",
        "\n",
        "    For regression: MSE, RMSE, MAE, RÂ²\n",
        "    For classification: Accuracy, Precision, Recall, F1\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    if problem_type == \"regression\":\n",
        "        # TODO: Calculate regression metrics\n",
        "        # TODO: Implement from scratch\n",
        "        mse = 0.0\n",
        "        rmse = 0.0\n",
        "        mae = 0.0\n",
        "        r2 = 0.0\n",
        "        return mse, rmse, mae, r2\n",
        "        pass\n",
        "    elif problem_type in [\"binary_classification\", \"multiclass_classification\"]:\n",
        "        # TODO: Calculate classification metrics\n",
        "        # TODO: Implement from scratch (no sklearn.metrics)\n",
        "        accuracy = 0.0\n",
        "        precision = 0.0\n",
        "        recall = 0.0\n",
        "        f1 = 0.0\n",
        "        return accuracy, precision, recall, f1\n",
        "        pass\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for both models\n",
        "# baseline_metrics = calculate_metrics(y_test, baseline_predictions, problem_type)\n",
        "# mlp_metrics = calculate_metrics(y_test, mlp_predictions, problem_type)\n",
        "\n",
        "print(\"Baseline Model Performance:\")\n",
        "# print(baseline_metrics)\n",
        "\n",
        "print(\"\\nMLP Model Performance:\")\n",
        "# print(mlp_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4SZCkf3BBug"
      },
      "source": [
        "## Section 6: Visualization\n",
        "\n",
        "Create visualizations:\n",
        "1. Training loss curves\n",
        "2. Performance comparison\n",
        "3. Additional domain-specific plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPqji5F0BBuh"
      },
      "outputs": [],
      "source": [
        "# 1. Training loss curves\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# TODO: Plot baseline loss\n",
        "# plt.plot(baseline_model.loss_history, label='Baseline', color='blue')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Baseline Model - Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# TODO: Plot MLP loss\n",
        "# plt.plot(mlp_model.loss_history, label='MLP', color='red')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('MLP Model - Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Pj6pQZBBuh"
      },
      "outputs": [],
      "source": [
        "# 2. Performance comparison bar chart\n",
        "# TODO: Create bar chart comparing key metrics between models\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Example:\n",
        "# metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "# baseline_scores = [baseline_metrics[m] for m in metrics]\n",
        "# mlp_scores = [mlp_metrics[m] for m in metrics]\n",
        "#\n",
        "# x = np.arange(len(metrics))\n",
        "# width = 0.35\n",
        "#\n",
        "# plt.bar(x - width/2, baseline_scores, width, label='Baseline')\n",
        "# plt.bar(x + width/2, mlp_scores, width, label='MLP')\n",
        "# plt.xlabel('Metrics')\n",
        "# plt.ylabel('Score')\n",
        "# plt.title('Model Performance Comparison')\n",
        "# plt.xticks(x, metrics)\n",
        "# plt.legend()\n",
        "# plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua107rhpBBuh"
      },
      "source": [
        "## Section 7: Analysis and Discussion\n",
        "\n",
        "Write your analysis (minimum 200 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fdNLJqCBBuh"
      },
      "outputs": [],
      "source": [
        "analysis_text = \"\"\"\n",
        "TODO: Write your analysis here (minimum 200 words)\n",
        "\n",
        "Address these questions:\n",
        "1. Which model performed better and by how much?\n",
        "2. Why do you think one model outperformed the other?\n",
        "3. What was the computational cost difference (training time)?\n",
        "4. Any surprising findings or challenges you faced?\n",
        "5. What insights did you gain about neural networks vs linear models?\n",
        "\n",
        "Write your thoughtful analysis here. Be specific and reference your actual results.\n",
        "Compare the metrics, discuss the trade-offs, and explain what you learned.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
        "if len(analysis_text.split()) < 200:\n",
        "    print(\"âš ï¸  Warning: Analysis should be at least 200 words\")\n",
        "else:\n",
        "    print(\"âœ“ Analysis meets word count requirement\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e3C2Bf4BBuh"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## â­ REQUIRED: Structured Output Function\n",
        "\n",
        "### **DO NOT MODIFY THE STRUCTURE BELOW**\n",
        "\n",
        "This function will be called by the auto-grader. Fill in all values accurately based on your actual results.\n",
        "\n",
        "\n",
        "â­â­â­ REQUIRED: Structured Output Function â­â­â­\n",
        "\n",
        "### ðŸš¨ CRITICAL - READ CAREFULLY ðŸš¨\n",
        "\n",
        "1. **Fill in ALL fields** - Missing fields = 0 marks\n",
        "2. **Use your actual values** - Not 0 or empty strings\n",
        "3. **This cell MUST be executed** - We need the output!\n",
        "4. **Print the results** - Auto-grader needs to see output!\n",
        "\n",
        "\n",
        "**DO NOT:**\n",
        "- Leave any field as 0, 0.0,\n",
        "- Clear outputs before submission\n",
        "- Modify the structure\n",
        "\n",
        "\n",
        "\"**MUST DO:**\n",
        "- Fill every field with your actual results\n",
        "- Execute this cell and keep the output\n",
        "- Print the results (see below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1W6qvYzBBuh"
      },
      "outputs": [],
      "source": [
        "def get_assignment_results():\n",
        "    '''\n",
        "    CRITICAL: Fill ALL fields with your actual results!\n",
        "    Missing fields will result in 0 marks for that section.\n",
        "    '''\n",
        "\n",
        "    results = {\n",
        "        # ===== Dataset Information (1 mark) =====\n",
        "        'dataset_name': dataset_name,  # MUST fill\n",
        "        'dataset_source': dataset_source,  # MUST fill\n",
        "        'n_samples': n_samples,  # MUST be â‰¥500\n",
        "        'n_features': n_features,  # MUST be â‰¥5\n",
        "        'problem_type': problem_type,  # MUST fill\n",
        "        'problem_statement': problem_statement,  # MUST be â‰¥50 words\n",
        "        'primary_metric': primary_metric,  # MUST fill\n",
        "        'metric_justification': metric_justification,  # MUST be â‰¥30 words\n",
        "        'train_samples': train_samples,\n",
        "        'test_samples': test_samples,\n",
        "        'train_test_ratio': train_test_ratio,\n",
        "\n",
        "        # ===== Baseline Model (3 marks) =====\n",
        "        'baseline_model': {\n",
        "            'model_type': '',  # 'linear_regression', 'logistic_regression', 'softmax_regression'\n",
        "            'learning_rate': 0.01,  # Your learning rate\n",
        "            'n_iterations': 1000,  # Your iterations\n",
        "\n",
        "            # CRITICAL: These MUST be filled!\n",
        "            'initial_loss': baseline_initial_loss,  # MUST NOT be 0\n",
        "            'final_loss': baseline_final_loss,  # MUST NOT be 0\n",
        "            'training_time_seconds': baseline_training_time,  # MUST NOT be 0\n",
        "            'loss_decreased': baseline_final_loss < baseline_initial_loss,  # Auto-calculated\n",
        "\n",
        "            # Metrics - Fill based on your problem type\n",
        "            'test_accuracy': 0.0 if problem_type == 'regression' else baseline_acc,\n",
        "            'test_precision': 0.0 if problem_type == 'regression' else baseline_prec,\n",
        "            'test_recall': 0.0 if problem_type == 'regression' else baseline_rec,\n",
        "            'test_f1': 0.0 if problem_type == 'regression' else baseline_f1,\n",
        "            'test_mse': baseline_mse if problem_type == 'regression' else 0.0,\n",
        "            'test_rmse': baseline_rmse if problem_type == 'regression' else 0.0,\n",
        "            'test_mae': baseline_mae if problem_type == 'regression' else 0.0,\n",
        "            'test_r2': baseline_r2 if problem_type == 'regression' else 0.0,\n",
        "        },\n",
        "\n",
        "        # ===== MLP Model (4 marks) =====\n",
        "        'mlp_model': {\n",
        "            'architecture': mlp_architecture,  # MUST have â‰¥3 elements\n",
        "            'n_hidden_layers': len(mlp_architecture) - 2 if len(mlp_architecture) > 0 else 0,\n",
        "            'learning_rate': 0.01,\n",
        "            'n_iterations': 1000,\n",
        "\n",
        "            # CRITICAL: These MUST be filled!\n",
        "            'initial_loss': mlp_initial_loss,  # MUST NOT be 0\n",
        "            'final_loss': mlp_final_loss,  # MUST NOT be 0\n",
        "            'training_time_seconds': mlp_training_time,  # MUST NOT be 0\n",
        "            'loss_decreased': mlp_final_loss < mlp_initial_loss,  # Auto-calculated\n",
        "\n",
        "            # Metrics\n",
        "            'test_accuracy': 0.0 if problem_type == 'regression' else mlp_acc,\n",
        "            'test_precision': 0.0 if problem_type == 'regression' else mlp_prec,\n",
        "            'test_recall': 0.0 if problem_type == 'regression' else mlp_rec,\n",
        "            'test_f1': 0.0 if problem_type == 'regression' else mlp_f1,\n",
        "            'test_mse': mlp_mse if problem_type == 'regression' else 0.0,\n",
        "            'test_rmse': mlp_rmse if problem_type == 'regression' else 0.0,\n",
        "            'test_mae': mlp_mae if problem_type == 'regression' else 0.0,\n",
        "            'test_r2': mlp_r2 if problem_type == 'regression' else 0.0,\n",
        "        },\n",
        "\n",
        "        # ===== Analysis (2 marks) =====\n",
        "        'analysis': analysis_text,\n",
        "        'analysis_word_count': len(analysis_text.split()),\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# ===== CRITICAL: CALL AND PRINT RESULTS =====\n",
        "# This MUST be executed and output MUST be visible!\n",
        "import json\n",
        "results = get_assignment_results()\n",
        "print(json.dumps(results, indent=2))\n",
        "\n",
        "# ===== Validation =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "errors = []\n",
        "\n",
        "if results['n_samples'] < 500:\n",
        "    errors.append(f\"âŒ Dataset too small: {results['n_samples']} < 500\")\n",
        "if results['n_features'] < 5:\n",
        "    errors.append(f\"âŒ Too few features: {results['n_features']} < 5\")\n",
        "if results['baseline_model']['initial_loss'] == 0:\n",
        "    errors.append(\"âŒ Baseline initial_loss is 0\")\n",
        "if results['baseline_model']['final_loss'] == 0:\n",
        "    errors.append(\"âŒ Baseline final_loss is 0\")\n",
        "if results['baseline_model']['training_time_seconds'] == 0:\n",
        "    errors.append(\"âŒ Baseline training_time is 0\")\n",
        "if results['mlp_model']['initial_loss'] == 0:\n",
        "    errors.append(\"âŒ MLP initial_loss is 0\")\n",
        "if results['mlp_model']['final_loss'] == 0:\n",
        "    errors.append(\"âŒ MLP final_loss is 0\")\n",
        "if results['mlp_model']['training_time_seconds'] == 0:\n",
        "    errors.append(\"âŒ MLP training_time is 0\")\n",
        "if len(results['mlp_model']['architecture']) < 3:\n",
        "    errors.append(\"âŒ MLP architecture invalid\")\n",
        "if results['analysis_word_count'] < 200:\n",
        "    errors.append(f\"âŒ Analysis too short: {results['analysis_word_count']} < 200 words\")\n",
        "\n",
        "if errors:\n",
        "    print(\"ERRORS FOUND:\")\n",
        "    for err in errors:\n",
        "        print(err)\n",
        "    print(\" FIX THESE BEFORE SUBMITTING! \")\n",
        "else:\n",
        "    print(\"âœ… All validation checks passed!\")\n",
        "    print(\"âœ… Ready to submit!\")\n",
        "    print(\"Next steps:\")\n",
        "    print(\"1. Kernel â†’ Restart & Clear Output\")\n",
        "    print(\"2. Kernel â†’ Restart & Run All\")\n",
        "    print(\"3. Verify this output is visible\")\n",
        "    print(\"4. Save notebook\")\n",
        "    print(\"5. Rename as: YourStudentID_assignment.ipynb\")\n",
        "    print(\"6. Submit to LMS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPvwotBWBBuh"
      },
      "source": [
        "## Test Your Output\n",
        "\n",
        "Run this cell to verify your results dictionary is complete and properly formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4mkH-nSBBui"
      },
      "outputs": [],
      "source": [
        "# Test the output\n",
        "import json\n",
        "\n",
        "try:\n",
        "    results = get_assignment_results()\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(json.dumps(results, indent=2))\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "\n",
        "    # Check for missing values\n",
        "    missing = []\n",
        "    def check_dict(d, prefix=\"\"):\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, dict):\n",
        "                check_dict(v, f\"{prefix}{k}.\")\n",
        "            elif (v == 0 or v == \"\" or v == 0.0 or v == []) and \\\n",
        "                 k not in ['improvement', 'improvement_percentage', 'baseline_better',\n",
        "                          'baseline_converged', 'mlp_converged', 'total_parameters',\n",
        "                          'test_accuracy', 'test_precision', 'test_recall', 'test_f1',\n",
        "                          'test_mse', 'test_rmse', 'test_mae', 'test_r2']:\n",
        "                missing.append(f\"{prefix}{k}\")\n",
        "\n",
        "    check_dict(results)\n",
        "\n",
        "    if missing:\n",
        "        print(f\"âš ï¸  Warning: {len(missing)} fields still need to be filled:\")\n",
        "        for m in missing[:15]:  # Show first 15\n",
        "            print(f\"  - {m}\")\n",
        "        if len(missing) > 15:\n",
        "            print(f\"  ... and {len(missing)-15} more\")\n",
        "    else:\n",
        "        print(\"âœ… All required fields are filled!\")\n",
        "        print(\"\\nðŸŽ‰ You're ready to submit!\")\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Kernel â†’ Restart & Clear Output\")\n",
        "        print(\"2. Kernel â†’ Restart & Run All\")\n",
        "        print(\"3. Verify no errors\")\n",
        "        print(\"4. Save notebook\")\n",
        "        print(\"5. Rename as: YourStudentID_assignment.ipynb\")\n",
        "        print(\"6. Submit to LMS\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error in get_assignment_results(): {str(e)}\")\n",
        "    print(\"\\nPlease fix the errors above before submitting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuRKuL80BBui"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ“¤ Before Submitting - Final Checklist\n",
        "\n",
        "- [ ] **All TODO sections completed**\n",
        "- [ ] **Both models implemented from scratch** (no sklearn models!)\n",
        "- [ ] **get_assignment_results() function filled accurately**\n",
        "- [ ] **Loss decreases for both models**\n",
        "- [ ] **Analysis â‰¥ 200 words**\n",
        "- [ ] **All cells run without errors** (Restart & Run All)\n",
        "- [ ] **Visualizations created**\n",
        "- [ ] **File renamed correctly**: YourStudentID_assignment.ipynb\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck! **"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
